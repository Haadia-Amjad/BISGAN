# BISGAN
This repository contains the evaluations and additional work done for the paper "Block Induced Signature Generative Adversarial Network (BISGAN): Signature Spoofing Using GANs and Their Evaluation"

# Preprocessing Component
Our preprocessing component takes inspiration from the work of Akhundjanov and Starovoitov. In their work, they have specifically designed a preprocessing pipeline for the CEDAR signature dataset which we have translated into our work.
The preprocessing component consists of 6 steps. In the initial stage, the Pixels Per Inch (PPI) are digitized to standardize resolution, followed by a conversion to grayscale to enable effective preprocessing. The image is then transformed into Band Interleaved by Pixel (BIP) format for efficient data handling, and a rectangle is cut out using Otsu binarization filters for improved segmentation. Principal Component Analysis (PCA) corrects rotation, and all signatures are scaled to 300x150 px, ensuring uniformity. The preprocessing is preferred to be done by using Python libraries Numpy, Sklearn, Natsort, Scipy, Glob and CV2.

![preproc ro](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/1a2c1d39-54a9-450a-9333-837d3cb06ecf)


# Training Visualization
To view our results and see real-time progress with learning, we use the Weight and Bases (wandb) platform. We create a dashboard, snippet shown in the figure below, where the image generation steps are visualized along with heat maps for identity loss, images generated by generator B using fake domain A images and vice versa. 
[wandbPDFcrop (1).pdf](https://github.com/user-attachments/files/16143983/wandbPDFcrop.1.pdf)

# Evaluations
1-Evaluation
Signature spoofing attempts to make a verification system unable to identify the forged signatures. As that is the goal of our model, the verification systems should perform poorly. We quantify this by analyzing the percentage of forged images that the verification system labels as genuine signatures. We brand this percentage as our success rate. 

For this experiment, we train four deep learning models on CEDAR, DeepSignDB, and SVC2021 EvalDB signature datasets to act as our verification systems. It is important to note that these datasets are small for classification learning and may impact results. Regardless, we stick with these datasets because BISGAN model is trained on them. Our verification systems are VGG-16 \cite{ref_lncs25}, AlexNet \cite{ref_lncs26}, SigNet-F \cite{ref_lncs27}, and CapsNet \cite{ref_lncs28} models, shown in Table \ref{tab:Table1}. Of these three, AlexNet performs the best during traditional training and testing.

Next, we generate ten (10) forgeries from the BISGAN model, shown in figure \ref{fig:imgsamp} and \ref{fig:bisgan}. Additionally, we train seven (7) other image generation models on CEDAR, SVC2021\_EvalDB and DeepSignDB signature datasets and generate 10 forgeries from all of these. This is also done to show the generation capabilities of BISGAN to further establish generalizability. Two (2) image generation models are based on techniques other than GANs to generate images, namely, RSAEG (perturbation-based) \cite{ref_article29} and the Diffusion model \cite{ref_lncs30}. Two (2) of them are GAN techniques that have not been used for signature generation, namely, MaskGIT \cite{ref_lncs31} and DCGAN \cite{ref_lncs32}. Three (3) of them are the latest GAN techniques used to generate signatures; CycleGAN, OSVGAN and Stroke-cCycleGAN. We pass the generated images of all the above architectures one by one as input to the four (4) verification systems that we have trained. We extract the success rate of all these architectures including our own, shown in Table \ref{tab:Table2}, Table \ref{tab:spoofotherdata} and Table \ref{tab:spooficdar}. 

We observe that our BISGAN with paradigm shift training performs the best towards our goal of signature spoofing followed closely by our normally trained BISGAN. The second and third successful techniques are Stroke-cCycleGAN and RSAEG respectively, shown in Figure \ref{fig:imgsamp} and \ref{fig:bisgan} \\

![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/473e0246-870e-4677-8126-2f71fcc19a2f)

![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/e787e21d-0620-4bfa-b0f1-85db8dcbf9d1)

![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/174adc79-5d4f-49c8-9f7d-7ae8f497c850

![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/19423042-57f6-4155-8837-0575e0acc92c)







2- Z Test
we combine 80 generated signatures with 80 original samples of CEDAR, SVC2021\_EvalDB and DeepSignDB collectively. We pass this test dataset to our four (4) verification systems and observe their performance. This experiment analyzes the overall performance of verification systems with quality forgeries in the dataset. In Table 
 \ref{tab:failall}, we see that our generated forgeries degrade the performance of verification systems and effectively bypass their identification abilities.
 \\

![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/49d8d73d-f810-4504-8fa3-c8fa19e51637)



 As mentioned, we generated 80 signatures per architecture for our experimentation. Since this number is small, it may raise questions regarding the presented results. To further validate the achievement of our model, we conduct a Z-test. A Z-test is a statistical hypothesis test used to determine whether a sample mean differs significantly from a known population mean (n $>$ 30). Based on the literature and our experiments, we have four main architectures that focus on generating spoofing-level forgeries: BISGAN (para), BISGAN, Stroke-CycleGAN, and OSVGAN. We conduct our test for two p-values at alpha ($\alpha$) 0.05 and 0.01. The null hypothesis in our experiment states that a simple CycleGAN architecture is better. Our experiments show that BISGAN rejects this hypothesis for all datasets, shown in Table \ref{tab:ztest}.

 
![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/7b74fe06-f3d7-4f6c-890a-40da0e9e7990)

3- Others:

As mentioned earlier in this section, traditional evaluation metrics are not particularly useful in our case as the purpose of our model is to fail verification models. However, plenty of metrics exist that speak to the quality of generated images in general. Similarity metrics also help in establishing an understanding of the closeness of a generated image to the original samples. We show our results in Table \ref{tab:Table4}. \\

The Inception Score \cite{ref_article35} assesses the quality and diversity of images generated by a deep learning model. It calculates two components: the image's quality (measured by the certainty of the model's predictions) and diversity (measured by the entropy of the predicted labels). A higher Inception Score indicates better image quality and diversity \cite{ref_article36}. Frechet Inception Distance \cite{ref_article37} measures the similarity between the feature representations of generated images and real images in a neural network's intermediate layer. A lower FID score suggests that the generated images are closer to real images in terms of features, indicating better quality. Kernel Inception Distance (KID) \cite{ref_lncs38} is an extension of FID that incorporates kernel methods to further enhance the evaluation of image quality and realism. A lower score indicates that the generated images are more similar to real images. Perceptual Path Length (PPL) \cite{ref_lncs39} evaluates the smoothness of transitions between generated images along a predefined path in the latent space. Lower values indicate smoother transitions, which are desirable for realistic and perceptually pleasing generative models. Structural Similarity (SSIM) \cite{ref_article40} measures the structural similarity between two images by comparing luminance, contrast, and structure. It provides a value between -1 and 1, where 1 indicates a perfect match. A higher SSIM score suggests better similarity between generated and real images.  Peak Signal-to-Noise Ratio (PSNR) \cite{ref_lncs41} is a widely used metric for assessing image quality, particularly in image and video compression. It quantifies the quality of a reconstructed image by comparing it to the original image. A higher PSNR value indicates better image quality, as it suggests that the reconstructed image is closer to the original with less noise. 
\\
![image](https://github.com/Haadia-Amjad/BISGAN/assets/175126707/5ff00d37-7905-4437-9501-a58faea7bc96)




